{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "tmp_catalog = '/home/jimmyli/announcement/'\n",
    "\n",
    "data_x = pd.read_csv(filepath_or_buffer='/home/jimmyli/announcement/bulletinu.csv', sep = ','\n",
    "                    )[\"公告级别\"].values\n",
    "data_y = pd.read_csv(filepath_or_buffer='/home/jimmyli/announcement/bulletinu.csv', sep = ','\n",
    "                    )[\"公告主题\"].values\n",
    "Hlength = len(data_x)\n",
    "\n",
    "f = open('/home/jimmyli/announcement/level_data.txt', 'w')\n",
    "\n",
    "for i in range(1, Hlength-1):\n",
    "    # print(data_x[i], data_y[i])\n",
    "    if (data_x[i] == '中'):\n",
    "        data_x[i] = '其它'\n",
    "    elif (data_x[i] == '高'):\n",
    "        data_x[i] = '重要'\n",
    "    f.write(str(data_x[i]))\n",
    "    f.write('\\t')\n",
    "    f.write(str(data_y[i]))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.491 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jimmyli/announcement/level_data.txt has been token and token_file_name is /home/jimmyli/announcement/level_token.txt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import jieba\n",
    "\n",
    "tmp_catalog = '/home/jimmyli/announcement/'\n",
    "\n",
    "def tokenFile(file_path, write_path):\n",
    "\n",
    "    with open(write_path, 'w') as w:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                token_sen = (\" \".join(jieba.cut(line.split('\\t')[1])))               \n",
    "                w.write(line.split('\\t')[0] + '\\t' + token_sen)\n",
    "    print(file_path + ' has been token and token_file_name is ' + write_path)\n",
    "\n",
    "tokenFile(tmp_catalog+'level_data.txt', tmp_catalog+'level_token.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(file_path, train_write_path, test_write_path):\n",
    "    \n",
    "    with open(train_write_path, 'w') as w:        \n",
    "        with open(test_write_path, 'w') as t:           \n",
    "            with open(file_path, 'r') as f:\n",
    "                flist = f.readlines()\n",
    "                for i in range(len(flist)):\n",
    "                    if i % 5 == 0:\n",
    "                        t.write(flist[i])\n",
    "                    else:\n",
    "                        w.write(flist[i])\n",
    "                    \n",
    "splitData(tmp_catalog+'level_token.txt', tmp_catalog+'level_train.txt',\n",
    "         tmp_catalog+'level_test.txt')                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructDataset(path):\n",
    "    \"\"\"\n",
    "    path: file path\n",
    "    rtype: lable_list and corpus_list\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    corpus_list = []\n",
    "    with open(path, 'r') as p:\n",
    "        for line in p:\n",
    "            label_list.append(line.split('\\t')[0])\n",
    "            corpus_list.append(line.split('\\t')[1])\n",
    "    return label_list, corpus_list\n",
    "\n",
    "file_path = ['level_test.txt', 'level_train.txt']\n",
    "test_label, test_set = constructDataset(tmp_catalog + file_path[0])   #448\n",
    "train_label, train_set = constructDataset(tmp_catalog + file_path[1]) #1788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus is:2236\n",
      "how many words: 2092\n",
      "tf-idf shape: (2236, 2092)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus_set = train_set + test_set\n",
    "print(\"length of corpus is:\" + str(len(corpus_set)))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1e-5) # drop df < le-5,去低频词\n",
    "\n",
    "transfomer = TfidfTransformer()\n",
    "\n",
    "tfidf = transfomer.fit_transform(vectorizer.fit_transform(corpus_set))\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"how many words: {0}\".format(len(words)))\n",
    "\n",
    "print(\"tf-idf shape: ({0}, {1})\".format(tfidf.shape[0], tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# encode label\n",
    "corpus_label = train_label + test_label\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "corpus_encode_label = encoder.fit_transform(corpus_label)\n",
    "train_label = corpus_encode_label[:1788]\n",
    "test_label = corpus_encode_label[1788:]\n",
    "# get tf-idf dataset\n",
    "train_set = tfidf[:1788]\n",
    "test_set = tfidf[1788:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.95      0.83       274\n",
      "          1       0.70      0.59      0.64        64\n",
      "          2       0.62      0.24      0.34       110\n",
      "\n",
      "avg / total       0.70      0.72      0.68       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# LogisticRegression classiy model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(train_set, train_label)\n",
    "y_pred = lr_model.predict(test_set)\n",
    "print (classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['其它', '极重要', '重要'], dtype='<U3')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
