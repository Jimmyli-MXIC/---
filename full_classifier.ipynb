{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "tmp_catalog = '/home/jimmyli/announcement/'\n",
    "\n",
    "data_x = pd.read_csv(filepath_or_buffer='/home/jimmyli/announcement/bulletinu.csv', sep = ','\n",
    "                    )[\"公告类别\"].values\n",
    "data_y = pd.read_csv(filepath_or_buffer='/home/jimmyli/announcement/bulletinu.csv', sep = ','\n",
    "                    )[\"公告主题\"].values\n",
    "Hlength = 833\n",
    "\n",
    "f = open('/home/jimmyli/announcement/data.txt', 'w')\n",
    "\n",
    "for i in range(1, Hlength):\n",
    "    # print(data_x[i], data_y[i])\n",
    "    f.write(str(data_x[i]))\n",
    "    f.write('\\t')\n",
    "    f.write(str(data_y[i]))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "将未识别图片的正文提取到content.txt中\n",
    "\"\"\"\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "#打开文件，用with打开可以不用去特意关闭file了，python3不支持file()打开文件，只能用open()\n",
    "with open(\"/home/jimmyli/announcement/content.txt\", 'w') as w:\n",
    "    with open(\"/home/jimmyli/bulletinu.csv\",\"r\",encoding=\"utf-8\") as csvfile:\n",
    "     #读取csv文件，返回的是迭代类型\n",
    "\n",
    "    \n",
    "        dictread=csv.DictReader(csvfile)\n",
    "    \n",
    "        for i ,row in enumerate(dictread):\n",
    "            if(i<833):\n",
    "            \n",
    "                html = row['公告内容']\n",
    "                bsObj= BeautifulSoup(html,\"lxml\")\n",
    "                w.write(bsObj.get_text())\n",
    "                w.write('\\n')\n",
    "\"\"\"\n",
    "将正文中的空格去除存到content_copy.txt\n",
    "\"\"\"\n",
    "import os \n",
    "\n",
    "with open((os.path.join('content.txt')), 'r') as f:\n",
    "    with open((os.path.join('content_copy.txt')), 'w') as w:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            odom = line.split()\n",
    "            tmp_str = \"\".join(odom)\n",
    "            result = ' '.join(tmp_str.split())\n",
    "            w.write(result)\n",
    "            w.write('\\n')\n",
    "\"\"\"\n",
    "将正文与标题结合生成data_full.txt\n",
    "\"\"\"\n",
    "tmp_catalog = '/home/jimmyli/announcement/'\n",
    "\n",
    "data = []\n",
    "content = []\n",
    "with open(tmp_catalog + 'data_full.txt', 'w') as w:\n",
    "    with open(tmp_catalog + 'data.txt', 'r') as t:\n",
    "        with open(tmp_catalog + 'content_copy.txt', 'r') as f:\n",
    "            for line in t:\n",
    "                line = line.rstrip('\\n')\n",
    "                data.append(line)\n",
    "            content = f.readlines()\n",
    "            for i in range(len(data)):\n",
    "                w.write(data[i] + ' ' + content[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jimmyli/announcement/data_full.txt has been token and token_file_name is /home/jimmyli/announcement/full_token.txt\n"
     ]
    }
   ],
   "source": [
    "'''分词,存放在full_token.txt'''\n",
    "import multiprocessing\n",
    "import jieba\n",
    "\n",
    "tmp_catalog = '/home/jimmyli/announcement/'\n",
    "\n",
    "def tokenFile(file_path, write_path):\n",
    "\n",
    "    with open(write_path, 'w') as w:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                token_sen = (\" \".join(jieba.cut(line.split('\\t')[1])))               \n",
    "                w.write(line.split('\\t')[0] + '\\t' + token_sen)\n",
    "    print(file_path + ' has been token and token_file_name is ' + write_path)\n",
    "\n",
    "tokenFile(tmp_catalog+'data_full.txt', tmp_catalog+'full_token.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(file_path, train_write_path, test_write_path):\n",
    "    \n",
    "    with open(train_write_path, 'w') as w:        \n",
    "        with open(test_write_path, 'w') as t:           \n",
    "            with open(file_path, 'r') as f:\n",
    "                flist = f.readlines()\n",
    "                for i in range(len(flist)):\n",
    "                    if i % 5 == 0:\n",
    "                        t.write(flist[i])\n",
    "                    else:\n",
    "                        w.write(flist[i])\n",
    "                    \n",
    "splitData(tmp_catalog+'full_token.txt', tmp_catalog+'full_train.txt',\n",
    "         tmp_catalog+'full_test.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664\n"
     ]
    }
   ],
   "source": [
    "def constructDataset(path):\n",
    "    \"\"\"\n",
    "    path: file path\n",
    "    rtype: lable_list and corpus_list\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    corpus_list = []\n",
    "    with open(path, 'r') as p:\n",
    "        for line in p:\n",
    "            label_list.append(line.split('\\t')[0])\n",
    "            corpus_list.append(line.split('\\t')[1])\n",
    "    return label_list, corpus_list\n",
    "\n",
    "file_path = ['full_test.txt', 'full_train.txt']\n",
    "test_label, test_set = constructDataset(tmp_catalog + file_path[0])   #167\n",
    "train_label, train_set = constructDataset(tmp_catalog + file_path[1]) #664\n",
    "print(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus is:831\n",
      "how many words: 3106\n",
      "tf-idf shape: (831, 3106)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus_set = train_set + test_set\n",
    "print(\"length of corpus is:\" + str(len(corpus_set)))\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1e-5) # drop df < le-5,去低频词\n",
    "\n",
    "transfomer = TfidfTransformer()\n",
    "\n",
    "tfidf = transfomer.fit_transform(vectorizer.fit_transform(corpus_set))\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"how many words: {0}\".format(len(words)))\n",
    "\n",
    "print(\"tf-idf shape: ({0}, {1})\".format(tfidf.shape[0], tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# encode label\n",
    "corpus_label = train_label + test_label\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "corpus_encode_label = encoder.fit_transform(corpus_label)\n",
    "train_label = corpus_encode_label[:664]\n",
    "test_label = corpus_encode_label[664:]\n",
    "# get tf-idf dataset\n",
    "train_set = tfidf[:664]\n",
    "test_set = tfidf[664:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         9\n",
      "          1       1.00      0.11      0.20         9\n",
      "          2       0.56      0.50      0.53        20\n",
      "          3       0.91      0.56      0.69        18\n",
      "          4       0.50      0.22      0.31        18\n",
      "          5       0.00      0.00      0.00         5\n",
      "          6       0.50      0.36      0.42        14\n",
      "          7       0.67      0.17      0.27        12\n",
      "          9       0.80      0.50      0.62        16\n",
      "         10       0.41      0.93      0.57        46\n",
      "\n",
      "avg / total       0.55      0.50      0.45       167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmyli/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# LogisticRegression classiy model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(train_set, train_label)\n",
    "y_pred = lr_model.predict(test_set)\n",
    "print (classification_report(test_label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GMO', 'IT Bulletin', '人力资源', '公共事务', '环境卫生安全', '社团活动', '福委会',\n",
       "       '设施服务', '财会税务', '资讯安全', '餐饮服务'], dtype='<U11')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
